import pandas as pd
import numpy as np
import re
from datetime import datetime
from collections import Counter
import tiktoken


# Clean the 'Date' column
df['Date'] = (
    df['Date']
    .astype(str)
    .str.replace(r"^b'|^b\"|\'$|\"$", "", regex=True)
    .str.strip()
    .replace(["nan", "NaN", "None", ""], np.nan)
)

# Assuming you have a chinese_digit_map like this
chinese_digit_map = {
    '一': '1', '二': '2', '三': '3', '四': '4', '五': '5', 
    '六': '6', '七': '7', '八': '8', '九': '9', '〇': '0'
}

date_str = "二〇二六年06月09日"

# Convert Chinese numerals to Arabic numerals
for ch, digit in chinese_digit_map.items():
    date_str = date_str.replace(ch, digit)



def extract_date_from_question(question):
    # Convert Chinese numerals to Arabic digits before regex
    raw_question = question
    lunar_patterns = [
        r"农历[〇零一二三四五六七八九十]{4}年[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?",
        r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?"
        r"农历[〇零一二三四五六七八九十]{4}年[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?",
        r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?",
        r"农历[〇零一二三四五六七八九十]{4}[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?"
    ]
    for pattern in lunar_patterns:
        match = re.search(pattern, raw_question)  # Use raw text
        if match:
            return match.group()

    # Now process Gregorian dates
    for ch, digit in chinese_digit_map.items():
        question = question.replace(ch, digit)
 
    # Now process other date formats
    converted = question
    for ch, digit in chinese_digit_map.items():
        converted = converted.replace(ch, digit)
    patterns = [
        r"(\d{4})\s*年\s*(\d{1,2})\s*月\s*(\d{1,2})\s*日", 
        r"[〇零一二三四五六七八九十]{4}年[一二三四五六七八九十]{1,3}月[一二三四五六七八九十]{1,3}日",  
        r"\b\d{1,2}月\d{1,2}日\b",
        r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九]{1,3}",
        r"(\d{4})\s*年\s*(\d{1,2})\s*月\s*(\d{1,2})\s*日",
    	r"\b\d{1,2}日\b",
        r"\d{1,2}\s*/\s*\d{1,2}\s*/\s*\d{4}",
    	r"\d{4}-\d{2}-\d{2}",
    	r"\b\d{2}/\d{2}/\d{4}\b",
    	r"\b\d{2}-\d{2}-\d{4}\b",
    	r"\b\d{4}/\d{2}/\d{2}\b",
    	r"\b\d{4}\.\d{2}\.\d{2}\b",
        r"\d{4}/\d{2}/\d{2}",
        r"\d{2}/\d{2}/\d{4}",
    	r"\b\d{8}\b"
        ]

    for pattern in patterns:
        match = re.search(pattern, question)
        if match:
            return match.group()
    return np.nan


df['Date'] = df.apply(
    lambda row: row['Date'] if pd.notna(row['Date']) else extract_date_from_question(str(row.get('Question', ''))),
    axis=1
)

def theta(tokenized_output, baseline):
    # Normalize tokens by stripping '▁' prefix
    norm = lambda tokens: [t.lstrip('▁') for t in tokens]
    t_norm = norm(tokenized_output)
    b_norm = norm(baseline)
    
    t_vals = Counter(t_norm)
    b_vals = Counter(b_norm)
    characters = list(t_vals.keys() | b_vals.keys())  # Unique tokens
    t_vect = [t_vals.get(char, 0) for char in characters]
    b_vect = [b_vals.get(char, 0) for char in characters]
    len_t = np.sqrt(sum(tv * tv for tv in t_vect))
    len_b = np.sqrt(sum(bv * bv for bv in b_vect))
    dot = sum(tv * bv for tv, bv in zip(t_vect, b_vect))
    denominator = len_t * len_b
    if denominator == 0:
        return 1.0
    return 1 - (dot / denominator)

chinese_digit_map = {
    '一': '1', '二': '2', '三': '3', '四': '4', '五': '5', 
    '六': '6', '七': '7', '八': '8', '九': '9', '〇': '0'
}

# Add this block for lunar month conversion
month_map = {
    "正": "1", "二": "2", "三": "3", "四": "4",
    "五": "5", "六": "6", "七": "7", "八": "8",
    "九": "9", "十": "10", "冬": "11", "腊": "12"
}

def baseline_tokenizer(date_str, format_type):
    try:
        date_str = re.sub(r"\s*([年月日])\s*", r"\1", date_str)
        
        if re.match(r"\d{4}年\d{1,2}月\d{1,2}日", date_str):
            y = re.search(r"(\d{4})年", date_str).group(1)
            m = re.search(r"(\d{1,2})月", date_str).group(1)
            d = re.search(r"(\d{1,2})日", date_str).group(1)
            return [y, "年", m, "月", d, "日"]

        elif re.match(r"农历", date_str):
            parts = re.split(r"(农历|年|月|日)", date_str)
            cleaned = [p for p in parts if p and p.strip()]

            if '月' in cleaned:
                month_idx = cleaned.index('月') - 1
                if month_idx >= 0:
                    month_part = cleaned[month_idx]
                 # Check if the part before '月' has 4+ characters (year + month)
                    if len(month_part) >= 4 and all(c in chinese_digit_map for c in month_part[:4]):
                        year_part = month_part[:4]
                        month_char = month_part[4:]
                        # Split into year and month parts
                        cleaned = cleaned[:month_idx] + [year_part, '年', month_char] + cleaned[month_idx+1:]
                        month_idx = cleaned.index('月') - 1  # Update month index after insertion
            
            # Convert month character to number
            if month_idx >=0:
                month_char = cleaned[month_idx]
                cleaned[month_idx] = month_map.get(month_char, month_char)
    
            return cleaned  # <-- CORRECTLY NESTED

        # ✅ New addition: Dash format

    # Handle YYYY-MM-DD
        if re.match(r"\d{4}-\d{2}-\d{2}", date_str):
            return [date_str[:4], "-", date_str[5:7], "-", date_str[8:]]
    # Handle DD/MM/YYYY
        elif re.match(r"\d{2}/\d{2}/\d{4}", date_str):
            return [date_str[6:], "/", date_str[3:5], "/", date_str[:2]]

        # Fallback for unrecognized formats
        else:
            return [date_str]

    except Exception as e:
        print(f"Tokenization error: {e}")
        return [date_str]  # Ensure always returns a list


LANGUAGE_CONFIG = {
    'Hausa': {
        'time_patterns': [
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*(?:na safe|na yamma|na dare|AM|PM|\(24hr\))\b',  # Matches "1 na safe" [[1]]
            r'\b\d{1,2}\s*(na safe|na yamma|na dare)\b',  # Matches "1 na safe" without colon [[2]]
            r'\b\d{1,2} ga (?:Janairu|Fabrairu|Maris|Afrilu|Mayu|Yuni|Yuli|Agusta|Satumba|Oktoba|Nuwamba|Disamba)\b',  # Detects "ga Month" [[3]]
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*\(?24hr\)?\b'  # Explicit 24-hour format [[4]]
        ],
        'timezone_patterns': [
            r'\b[A-Za-z]+/[A-Za-z_]+\b',  # Matches "Amurka/Mountain" [[5]]
            r'\bUTC[+-]?\d{2}:?\d{2}\b',  # Matches "UTC+8" [[6]]
            r'\b(WAT|EAT|CAT)\b'          # Hausa-specific timezone abbreviations [[7]]
        ]
    },
    'German': {
        'time_patterns': [
            r'\b\d{1,2}[:\.]\d{2}\s*Uhr\b',  # German 18:00 Uhr [[8]]
            r'\b\d{1,2}\s*(Uhr|h)\b',        # Simplified time [[9]]
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*\(?24Uhr\)?\b'  # 24-hour format [[10]]
        ],
        'timezone_patterns': [
            r'\b[A-Za-z]+/[A-Za-z_]+\b',  # Matches "Europe/Berlin" [[11]]
            r'\bMEZ|MESZ\b',              # German timezone abbreviations [[12]]
            r'\bUTC[+-]?\d{2}:?\d{2}\b'   # UTC+01:00 [[13]]
        ]
    },
    'Chinese': {
        'time_patterns': [
            r'(上午|下午|晚间)\s*\d{1,2}(点\d{1,2}(分\d{1,2})?)?',  # 上午3点15分 [[14]]
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*\(?24小时\)?\b'         # 18:00 (24hr) [[15]]
        ],
        'timezone_patterns': [
            r'\b[A-Za-z]+/[A-Za-z_]+\b',  # Matches "Asia/Shanghai" [[16]]
            r'(上午|下午|晚间)\s*\d{1,2}([:点]\d{1,2}([:分]\d{1,2})?)?',
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*\(?24小时\)?\b'
            r'\b北京时间|中国标准时间\b',  # 北京时间 [[17]]
            r'\bUTC\+?8\b'               # UTC+8 [[18]]
        ]
    },
    'English': {
        'time_patterns': [
            r'\b\d{1,2}:\d{2}(?::\d{2})?\s*(?:AM|PM|24hr)\b',  # 6:30 PM [[1]]
            r'\b\d{1,2}\s*(?:AM|PM)\b'                          # 6 PM [[2]]
        ],
        'timezone_patterns': [
            r'\b[A-Za-z]+/[A-Za-z_]+\b',  # Matches "US/Hawaii" [[5]]
            r'\bUTC|GMT|EST|PST|CET\b',   # Timezone abbreviations [[19]]
            r'\bUTC[+-]?\d{2}:?\d{2}\b'   # UTC+8 [[20]]
        ]
    }
}

def detect_language(question):
    text = question.lower()
    if any(re.search(p, text, flags=re.IGNORECASE) for p in LANGUAGE_CONFIG['Hausa']['time_patterns']):
        return 'Hausa'
    if any(re.search(p, text, flags=re.IGNORECASE) for p in LANGUAGE_CONFIG['German']['time_patterns']):
        return 'German'
    if any(re.search(p, text, flags=re.IGNORECASE) for p in LANGUAGE_CONFIG['Chinese']['time_patterns']):
        return 'Chinese'
    return 'English'  # Fallback to English [[2]]

def detect_timezone(text, language="Chinese"):
    # Match timezone markers + clock times
    patterns = [
    r'(上午|下午|凌晨|晚上)\s*\d{1,2}点\d{1,2}分?',  # Matches "晚上10点15分"
    r'\b\d{1,2}:\d{2}(?::\d{2})?\s*(?:24小时|24hr)\b',  # Matches "18:00 (24小时)"
    r'\b\d{1,2}\s*(?:点|时)\b'  # Matches "10点"

    ]
    return any(re.search(p, text) for p in patterns)

def detect_calendar(text):
    if re.search(r"农历", text):
        return 'Lunar'
    if re.search(r"[-/]", text):
        return 'Gregorian'
    if '年' in text and '月' in text:
        return 'Chinese'  # Fallback
    return 'Gregorian'

def extract_year(date_str):
    # Lunar year extraction
    lunar_match = re.search(r"农历([〇零一二三四五六七八九十]{4})", date_str)
    if lunar_match:
        raw_year = lunar_match.group(1)
        converted_year = ''.join([chinese_digit_map.get(c, c) for c in raw_year])
        return int(converted_year) if converted_year.isdigit() else None

    # Gregorian year extraction
    converted = ''.join([chinese_digit_map.get(c, c) for c in date_str])
    converted = re.sub(r"\s+", "", date_str)  # Remove all spaces
    converted = re.sub(r"\s+", "", date_str)  # Remove spaces
    formats = [
        r"(\d{4})年",  # YYYY年
        r"\b(\d{4})[-/]",  # YYYY-MM or YYYY/MM
        r"\b\d{2}[/-]\d{2}[/-](\d{4})\b"  # DD/MM/YYYY
    ]
    for fmt in formats:
        match = re.search(fmt, converted)
        if match:
            return int(match.group(1))
    return None



def label_period(year, date_str=None):
    if not year:
        return "Unknown", "Unknown"
    century_num = (year - 1) // 100 + 1
    suffix = 'th' if 10 <= century_num % 100 <= 20 else {1: 'st', 2: 'nd', 3: 'rd'}.get(century_num % 10, 'th')
    if year < 2000:
        return "Historical (Pre-2000)", f"{century_num}{suffix} Century"
    elif 2000 <= year <= 2025:
        return "Contemporary (2000–2025)", f"{century_num}{suffix} Century"
    else:
        return "Future (Post-2026)", f"{century_num}{suffix} Century"

def analyze_token_semantics(tokenized_output, correct_tokens):
    analysis = {
        'Token Count': len(tokenized_output),
        'Splits Components': False,
        'Preserves Separators': True,  # Initialize as True
        'Semantic Integrity': 1.0
    }

    # Normalize tokens (strip '▁' prefix)
    normalized_model = [t.lstrip('▁') for t in tokenized_output]
    normalized_baseline = [t.lstrip('▁') for t in correct_tokens]

    # Penalty 1: Splits Components
    analysis['Splits Components'] = normalized_model != normalized_baseline

    # Penalty 2: Preserves Separators
    expected_separators = {'年', '月', '日', '-', '/', '.', ' '}
    expected_separators = {'年', '月', '日', '-', '/', '.', ' ', '农历'}  # Added 农历
    preserved = True
    for sep in expected_separators:
        if sep in normalized_baseline and sep not in normalized_model:
            preserved = False
            break
    analysis['Preserves Separators'] = preserved

    # Penalty 3: Token Count Difference
    token_count_penalty = 0.05 * abs(len(tokenized_output) - len(correct_tokens))

    # Penalty 4: Theta Score (Cosine Dissimilarity)
    theta_score = theta(tokenized_output, correct_tokens)

    # Calculate SI
    analysis['Semantic Integrity'] -= (
        0.1 * analysis['Splits Components'] +
        0.1 * (not analysis['Preserves Separators']) +
        token_count_penalty +
        theta_score
    )
    analysis['Semantic Integrity'] = max(0.0, min(1.0, analysis['Semantic Integrity']))

    return analysis


def infer_date_format(date_str):
    if re.match(r"农历[〇零一二三四五六七八九十]{4}年", date_str):
        return "Lunar (YYYY年MM月DD日)"
    if re.match(r"农历[〇零一二三四五六七八九十]{4}[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?", date_str):
        return "Lunar (YYYY年MM月DD日)"
    elif re.match(r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三]", date_str):
        return "Lunar (MM月DD日)"
    elif re.match(r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}", date_str):
        return "Lunar (MM月DD日)"
    if re.match(r"\d{4}\s*年\s*\d{1,2}\s*月\s*\d{1,2}\s*日", date_str):
        return "YYYY年MM月DD日"
    elif re.match(r"[二〇零一二三四五六七八九十]{4}年[一二三四五六七八九十]{1,2}月[一二三四五六七八九十]{1,3}日", date_str):
        return "Chinese Numerals YYYY年MM月DD日"  # ✅ New pattern 1
    elif re.match(r"农历[正一二三四五六七八九十冬腊]{1,2}月[初十廿三一二三四五六七八九卅]{1,3}日?", date_str):
        return "Lunar Date (MM月DD日)"
    elif re.match(r"\d{1,2}\s*月\s*\d{1,2}\s*日", date_str):
        return "MM月DD日"
    elif re.match(r"\d{4}-\d{2}-\d{2}", date_str):
        return "YYYY-MM-DD"
    elif re.match(r"\d{2}/\d{2}/\d{4}", date_str):
        return "DD/MM/YYYY"
    elif re.match(r"\d{2}-\d{2}-\d{4}", date_str):
        return "DD-MM-YYYY"
    elif re.match(r"\d{4}/\d{2}/\d{2}", date_str):
        return "YYYY/MM/DD"
    elif re.match(r"\d{4}\.\d{2}\.\d{2}", date_str):
        return "YYYY.MM.DD"
    elif re.match(r"\d{8}", date_str):
        return "YYYYMMDD"
    return "Unknown"

# GPT tokenization models
model_encoding_map = {
    "gpt-4": "cl100k_base",
    "gpt-4o": "o200k_base"
}

for model_name, encoding_name in model_encoding_map.items():
    print(f"Processing model: {model_name}")
    encoding = tiktoken.get_encoding(encoding_name)
    results = []

    for _, row in df.iterrows():
        question = str(row.get("Question", "")).strip()
        date_str = extract_date_from_question(question)
        if pd.isna(date_str): date_str = ""

        fmt = infer_date_format(date_str)
        year = extract_year(date_str)
        period, century = label_period(year)

        token_ids = encoding.encode(date_str)
        tokens = [encoding.decode_single_token_bytes(t).decode("utf-8", errors="replace") for t in token_ids]
        correct_tokens = baseline_tokenizer(date_str, fmt)
        analysis = analyze_token_semantics(tokens, correct_tokens)

        results.append({
            "Model": model_name,
            "Date Format": fmt,
            "Year": year,
            "Time Period": period,
            "Century": century,
            "Token Count": analysis["Token Count"],
            "Tokenized Output": " ".join(tokens),
            "TokenSequence": str(tokens),
            "Semantic Integrity": analysis["Semantic Integrity"],
            "Splits Components": analysis["Splits Components"],
            "Preserves Separators": analysis["Preserves Separators"],
            "Timezone Mentioned": detect_timezone(question),
            "Calendar Type Detected": detect_calendar(date_str),
            "Question": question,
            "Date": date_str
        })

    output_df = pd.DataFrame(results)
    safe_name = model_name.replace("/", "_")
    output_df.to_csv(f"{safe_name}_English_results.csv", index=False, encoding="utf-8-sig")
    print(f"Saved results for {model_name}!")
